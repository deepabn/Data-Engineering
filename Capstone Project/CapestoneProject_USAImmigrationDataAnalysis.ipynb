{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Analysis of United States Immigration Data and Creating Spark-warehouse \n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The goal of the project is to create a spark-warehouse with fact and dimension tables in star schema. This objective is achieved by integrating data from different data sources for the purpose of data analysis and future backened querying. Additionally, the U.S Customs & Border Protection Department could leverage this source-of-truth database to open the solution through a web API so backend web services could query the warehouse for information relating to international visitors.\n",
    "\n",
    "The project follows the following steps:\n",
    "* Step 1: Describes the scope of the Project and gathers data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Defines the Data Model\n",
    "* Step 4: Runs ETL to Model the Data\n",
    "* Step 5: Completes Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import configparser\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, DoubleType, LongType, StringType\n",
    "import datetime\n",
    "from pyspark.sql.functions import date_add, when, lower, isnull, dayofmonth, hour, weekofyear, dayofweek, date_format, udf, col, lit, year, month, upper, to_date\n",
    "from pyspark.sql.functions import lit, udf, col\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import timedelta, datetime\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The goal of the project is to integrate different types of data sources to create a spark-warehouse of fact and dimension tables in Star Schema. This star schema model will help the BI and Data Analysts to query the database schema with minimum number of SQL JOINs, therby increasing the prformance.\n",
    "\n",
    "##### Data Sources:\n",
    "   * **I94 Immigration Dataset:** This data comes from the US National Tourism and Trade Office. Source: [I94 Immigration Data](https://www.trade.gov/national-travel-and-tourism-office)\n",
    "   * **World Temperature Data:** This dataset came from Kaggle. Source: [World Temperature Data](https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data)\n",
    "   * **U.S. City Demographic Data:** This data comes from OpenSoft. Source: [U.S. City Demographic Data](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "\n",
    "The descriptions contained in I94_SAS_Labels_Descriptions.SAS file will also be utilized.\n",
    "\n",
    "##### Tools and Pre-requisites:\n",
    "* For performing Exploratory data analysis on small dataset, Pandas library is used.It is helpful to efficiently load and manipulate data\n",
    "* For performing Exploratory data analysis on large dataset, Pyspark library is used. In future, we can also think of using Spark available within Amazon EMR for distributed processing\n",
    "* Jupyter Notebook is used to show the data structure and the need for data cleaning. Language used is Python since I am most comfortable with this language. For assessing the data, I have also used SQL language.\n",
    "* configparser python 3 is needed to run the python scripts\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "* **I94 Immigration Data Set:** U.S. immigration officers issued the I-94 Form (Arrival/Departure Record) to foreign visitors (e.g., business visitors, tourists and foreign students) who lawfully entered the United States. The I-94 is a white paper form that a foreign visitor received from cabin crews on arrival flights and from U.S. Customs and Border Protection at the time of entry into the United States. It listed the traveler's immigration category, port of entry, data of entry into the United States, status expiration date and had a unique 11-digit identifying number assigned to it. Its purpose is to record the traveler's lawful admission to the United States. This is the main dataset and there is a file for each month of the year of 2016 available in the directory ../../data/18-83510-I94-Data-2016/ in the SAS binary database storage format sas7bdat. When combined, the 12 datasets have than 40 million rows (40,790,529) and 28 columns. For most of the work, only the month of April of 2016 data is used which has more than three million records (3,096,313).\n",
    "\n",
    "* **World Temperature Data:** The dataset provides a long period of the world's montly average temperature (from year 1743 to 2013) at different country in the world wide. The Berkeley Earth, which is affiliated with Lawrence Berkeley National Laboratory, has repackaged the data from a newer compilation. The Berkeley Earth Surface Temperature Study combines 1.6 billion temperature reports from 16 pre-existing archives. It is nicely packaged and allows for slicing into interesting subsets (for example by country). They publish the source data and the code for the transformations they applied. They also use methods that allow weather observations from shorter time series to be included, meaning fewer observations need to be thrown away. In the original dataset from Kaggle, several files are available but in this capstone project we will be using only the GlobalLandTemperaturesByCity.csv\n",
    "\n",
    "* **U.S. City Demographic Data:** This data comes from the US Census Bureau's 2015 American Community Survey. This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. We will be using us-cities-demographics.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Reading immigration dataset here: \n",
    "The immigration dataset is large, containing almost 3 million records, hence have will use a subset of approx 1000 rows in a csv to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_subset = pd.read_csv(\"immigration_data_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>20568.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160423</td>\n",
       "      <td>MTR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20571.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160428</td>\n",
       "      <td>DOH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Z</td>\n",
       "      <td>K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>721257</td>\n",
       "      <td>1481650.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20552.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GA</td>\n",
       "      <td>20606.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>10072016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DL</td>\n",
       "      <td>7.368526e+08</td>\n",
       "      <td>910</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1072780</td>\n",
       "      <td>2197173.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>SFR</td>\n",
       "      <td>20556.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20635.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>10112016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CX</td>\n",
       "      <td>7.863122e+08</td>\n",
       "      <td>870</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>112205</td>\n",
       "      <td>232708.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20546.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20554.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>06302016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BA</td>\n",
       "      <td>5.547449e+10</td>\n",
       "      <td>00117</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2577162</td>\n",
       "      <td>5227851.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>IL</td>\n",
       "      <td>20575.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>07262016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LX</td>\n",
       "      <td>5.941342e+10</td>\n",
       "      <td>00008</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10930</td>\n",
       "      <td>13213.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>06292016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>5.544979e+10</td>\n",
       "      <td>00109</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "5      721257  1481650.0  2016.0     4.0   577.0   577.0     ATL  20552.0   \n",
       "6     1072780  2197173.0  2016.0     4.0   245.0   245.0     SFR  20556.0   \n",
       "7      112205   232708.0  2016.0     4.0   113.0   135.0     NYC  20546.0   \n",
       "8     2577162  5227851.0  2016.0     4.0   131.0   131.0     CHI  20572.0   \n",
       "9       10930    13213.0  2016.0     4.0   116.0   116.0     LOS  20545.0   \n",
       "\n",
       "   i94mode i94addr  depdate  i94bir  i94visa  count  dtadfile visapost occup  \\\n",
       "0      1.0      HI  20573.0    61.0      2.0    1.0  20160422      NaN   NaN   \n",
       "1      1.0      TX  20568.0    26.0      2.0    1.0  20160423      MTR   NaN   \n",
       "2      1.0      FL  20571.0    76.0      2.0    1.0  20160407      NaN   NaN   \n",
       "3      1.0      CA  20581.0    25.0      2.0    1.0  20160428      DOH   NaN   \n",
       "4      3.0      NY  20553.0    19.0      2.0    1.0  20160406      NaN   NaN   \n",
       "5      1.0      GA  20606.0    51.0      2.0    1.0  20160408      NaN   NaN   \n",
       "6      1.0      CA  20635.0    48.0      2.0    1.0  20160412      NaN   NaN   \n",
       "7      1.0      NY  20554.0    33.0      2.0    1.0  20160402      NaN   NaN   \n",
       "8      1.0      IL  20575.0    39.0      2.0    1.0  20160428      NaN   NaN   \n",
       "9      1.0      CA  20553.0    35.0      2.0    1.0  20160401      NaN   NaN   \n",
       "\n",
       "  entdepa entdepd  entdepu matflag  biryear   dtaddto gender  insnum airline  \\\n",
       "0       G       O      NaN       M   1955.0  07202016      F     NaN      JL   \n",
       "1       G       R      NaN       M   1990.0  10222016      M     NaN     *GA   \n",
       "2       G       O      NaN       M   1940.0  07052016      M     NaN      LH   \n",
       "3       G       O      NaN       M   1991.0  10272016      M     NaN      QR   \n",
       "4       Z       K      NaN       M   1997.0  07042016      F     NaN     NaN   \n",
       "5       T       N      NaN       M   1965.0  10072016      M     NaN      DL   \n",
       "6       T       O      NaN       M   1968.0  10112016      F     NaN      CX   \n",
       "7       G       O      NaN       M   1983.0  06302016      F     NaN      BA   \n",
       "8       O       O      NaN       M   1977.0  07262016    NaN     NaN      LX   \n",
       "9       O       O      NaN       M   1981.0  06292016    NaN     NaN      AA   \n",
       "\n",
       "         admnum  fltno visatype  \n",
       "0  5.658267e+10  00782       WT  \n",
       "1  9.436200e+10  XBLNG       B2  \n",
       "2  5.578047e+10  00464       WT  \n",
       "3  9.478970e+10  00739       B2  \n",
       "4  4.232257e+10   LAND       WT  \n",
       "5  7.368526e+08    910       B2  \n",
       "6  7.863122e+08    870       B2  \n",
       "7  5.547449e+10  00117       WT  \n",
       "8  5.941342e+10  00008       WT  \n",
       "9  5.544979e+10  00109       WT  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "df_immigration_subset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port',\n",
       "       'arrdate', 'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa',\n",
       "       'count', 'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd',\n",
       "       'entdepu', 'matflag', 'biryear', 'dtaddto', 'gender', 'insnum',\n",
       "       'airline', 'admnum', 'fltno', 'visatype'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration_subset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The definition of these fields is included in the file I94_SAS_Labels_Descriptions.SAS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Reading global temperature dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_global_temperature = pd.read_csv('../../data2/GlobalLandTemperaturesByCity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8599212, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_global_temperature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_global_temperature.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Reading demographics dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics = pd.read_csv('us-cities-demographics.csv', delimiter = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['City', 'State', 'Median Age', 'Male Population', 'Female Population',\n",
       "       'Total Population', 'Number of Veterans', 'Foreign-born',\n",
       "       'Average Household Size', 'State Code', 'Race', 'Count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "* To explore the dataset which is in SAS binary database storage format - sas7bdat, Pyspark library's SparkSession will be utilized\n",
    "* Datasets have been explored using pandas dataframe and sql\n",
    "* These datasets will be used to create fact and dimension tables. Hence data set splitting will be performed and various cleaning steps will be taken into account based on the Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Exploring Immigration dataset using Spark: i94_apr16_sub.sas7bdat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_immigration = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cicid=5748517.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='CA', depdate=20582.0, i94bir=40.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1976.0, dtaddto='10292016', gender='F', insnum=None, airline='QF', admnum=94953870030.0, fltno='00011', visatype='B1'),\n",
       " Row(cicid=5748518.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='NV', depdate=20591.0, i94bir=32.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1984.0, dtaddto='10292016', gender='F', insnum=None, airline='VA', admnum=94955622830.0, fltno='00007', visatype='B1')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#write to parquet\n",
    "# df_immigration.write.parquet(\"sas_data\")\n",
    "df_immigration = spark.read.parquet(\"sas_data\")\n",
    "df_immigration.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration.createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT cicid)|\n",
      "+---------------------+\n",
      "|              3096313|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirming if 'cicid' column has unique values\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT cicid)\n",
    "FROM immigration_table\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|i94Port_code_length|\n",
      "+-------------------+\n",
      "|                  3|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In the provided \"I94_SAS_Labels_Descriptions.SAS\" file, i94port is a code of three character long.\n",
    "# Confirming if the same applies to the \"immigration_table\"\n",
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT LENGTH(i94port) AS i94Port_code_length\n",
    "FROM immigration_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|distinct_i94visa|\n",
      "+----------------+\n",
      "|             1.0|\n",
      "|             2.0|\n",
      "|             3.0|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirming the distinct values present within \"i94visa\" column\n",
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT i94visa AS distinct_i94visa\n",
    "FROM immigration_table\n",
    "ORDER BY distinct_i94visa\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As per the \"I94_SAS_Labels_Descriptions.SAS\" file info, The three categories are:\n",
    "\n",
    "* 1 = Business\n",
    "* 2 = Pleasure\n",
    "* 3 = Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|i94mode|count(i94mode)|\n",
      "+-------+--------------+\n",
      "|   null|             0|\n",
      "|    1.0|       2994505|\n",
      "|    3.0|         66660|\n",
      "|    2.0|         26349|\n",
      "|    9.0|          8560|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirming the types of values in \"i94mode\" column\n",
    "spark.sql(\"\"\"\n",
    "SELECT i94mode, COUNT(i94mode)\n",
    "FROM immigration_table\n",
    "GROUP BY i94mode\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The i94mode definition as per the dictonary is as mentioned below.We see that almost majority of the data is related to \"Air\" travel, ie. appr 2994505 records. We do not have to ignore the other modes, since we might need to keep that also in our datawarehouse for future reference and statistics.\n",
    "\n",
    "* 1 = 'Air'\n",
    "* 2 = 'Sea'\n",
    "* 3 = 'Land'\n",
    "* 9 = 'Not reported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     802|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking for null values in \"i94bir\" column\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM immigration_table\n",
    "WHERE i94bir IS NULL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|count(biryear)|\n",
      "+--------------+\n",
      "|             0|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Since we have null values in \"i94bir\" column checking for null values in \"biryear\" column\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(biryear) FROM immigration_table WHERE biryear IS NULL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|gender|gender_count|\n",
      "+------+------------+\n",
      "|     F|     1302743|\n",
      "|  null|           0|\n",
      "|     M|     1377224|\n",
      "|     U|         467|\n",
      "|     X|        1610|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirming if the \"gender\" column is useable\n",
    "spark.sql(\"\"\"\n",
    "SELECT gender, COUNT(gender) AS gender_count\n",
    "FROM immigration_table\n",
    "GROUP BY gender\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We analyse that most of the records fall into Female and Male categories, and some of the immigrants do not wish to reveal the gender. There are no NULL values in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|i94cit_null_count|\n",
      "+-----------------+\n",
      "|                0|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|i94res_null_count|\n",
      "+-----------------+\n",
      "|                0|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirming citizenship-\"i94cit\" and residence-\"i94res\" columns to see if any values are missing\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*) AS i94cit_null_count\n",
    "FROM immigration_table\n",
    "WHERE i94cit IS NULL\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*) AS i94res_null_count\n",
    "FROM immigration_table\n",
    "WHERE i94res IS NULL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|i94cit_len|\n",
      "+----------+\n",
      "|         3|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|i94res_len|\n",
      "+----------+\n",
      "|         3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirming the length of i94cit and i94res columns\n",
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT LENGTH(CAST(i94cit AS BIGINT)) AS i94cit_len\n",
    "FROM immigration_table\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT LENGTH(CAST(i94res AS BIGINT)) AS i94res_len\n",
    "FROM immigration_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|visatype_null_count|\n",
      "+-------------------+\n",
      "|                  0|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the relevence of \"visatype\" column\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*) AS visatype_null_count\n",
    "FROM immigration_table\n",
    "WHERE visatype IS NULL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|visatype|count(visatype)|\n",
      "+--------+---------------+\n",
      "|      B1|         212410|\n",
      "|      B2|        1117897|\n",
      "|      CP|          14758|\n",
      "|     CPL|             10|\n",
      "|      E1|           3743|\n",
      "|      E2|          19383|\n",
      "|      F1|          39016|\n",
      "|      F2|           2984|\n",
      "|     GMB|            150|\n",
      "|     GMT|          89133|\n",
      "|       I|           3176|\n",
      "|      I1|            234|\n",
      "|      M1|           1317|\n",
      "|      M2|             49|\n",
      "|     SBP|             11|\n",
      "|      WB|         282983|\n",
      "|      WT|        1309059|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Studying the distint values in \"visatype\" column\n",
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT visatype, COUNT(visatype)\n",
    "FROM immigration_table\n",
    "GROUP BY visatype\n",
    "ORDER BY visatype\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since the I94_SAS_Labels_Descriptions.SAS file does not provide more information about different visa types, reference here https://travel.state.gov/content/travel/en/us-visas/visa-information-resources/all-visa-categories.html for detailed information. We will surely take this column into account since it might be useful for future statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|occup|occup_count|\n",
      "+-----+-----------+\n",
      "| null|    3088187|\n",
      "|  STU|       4719|\n",
      "|  OTH|        661|\n",
      "|  NRR|        345|\n",
      "|  MKT|        280|\n",
      "|  EXA|        196|\n",
      "|  GLS|        189|\n",
      "|  ULS|        175|\n",
      "|  ADM|        125|\n",
      "|  TIE|        124|\n",
      "|  MVC|        110|\n",
      "|  ENO|         60|\n",
      "|  CEO|         56|\n",
      "|  TIP|         52|\n",
      "|  RET|         50|\n",
      "|  CMP|         47|\n",
      "|  LLJ|         46|\n",
      "|  PHS|         45|\n",
      "|  UNP|         45|\n",
      "|  HMK|         40|\n",
      "+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analysing the relevance of \"occup\" column\n",
    "spark.sql(\"\"\"\n",
    "SELECT occup, COUNT(*) occup_count\n",
    "FROM immigration_table\n",
    "GROUP BY occup\n",
    "ORDER BY occup_count DESC, occup\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We find that majority of records have null in \"occup\" column. And for the others, the abbreviations are not useful, hence we will not consider this column in our data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|insnum|insnum_count|\n",
      "+------+------------+\n",
      "|  null|     2982605|\n",
      "|  3692|        2155|\n",
      "|  3697|        2033|\n",
      "|  3703|        1986|\n",
      "|  3893|        1866|\n",
      "|  3661|        1820|\n",
      "|  3693|        1690|\n",
      "|  3939|        1680|\n",
      "|  3672|        1678|\n",
      "|  3882|        1673|\n",
      "|  3943|        1662|\n",
      "|  3679|        1635|\n",
      "|  3949|        1584|\n",
      "|  3945|        1568|\n",
      "|  3691|        1535|\n",
      "|  3690|        1488|\n",
      "|  3954|        1470|\n",
      "|  3694|        1450|\n",
      "|  3890|        1394|\n",
      "|  3941|        1331|\n",
      "+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analysing the relevance of insnum column\n",
    "spark.sql(\"\"\"\n",
    "SELECT insnum, COUNT(*) as insnum_count\n",
    "FROM immigration_table\n",
    "GROUP BY insnum\n",
    "ORDER BY insnum_count DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Analysed, majority of records have null in \"insnum\" column. And for the others, the numbers are not useful, hence we will not consider this column in our data model\n",
    "\n",
    "All the remaining columns have been ignored since they have lot of missing values and also we do not have relevant documentation details about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Exploring Global Temperature dataset: GlobalLandTemperaturesByCity.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_global_temperature['Country'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This dataset has recorded 159 unique countries temperature information, since the year 1743, but we are interested only in the \"United States\" data hence we will be filtering it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2013-09-01'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the most recent date in the dataset\n",
    "df_global_temperature['dt'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Analysed that no temperature that's available here can be joined with our immigration dataset. But we will include this in our data model since we might get new data in the future and hence we will be able to join it with the immigration dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>402597</th>\n",
       "      <td>1950-02-01</td>\n",
       "      <td>11.144</td>\n",
       "      <td>0.199</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>96.70W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405836</th>\n",
       "      <td>1950-02-01</td>\n",
       "      <td>1.655</td>\n",
       "      <td>0.057</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>United States</td>\n",
       "      <td>39.38N</td>\n",
       "      <td>76.99W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                dt  AverageTemperature  AverageTemperatureUncertainty  \\\n",
       "402597  1950-02-01              11.144                          0.199   \n",
       "405836  1950-02-01               1.655                          0.057   \n",
       "\n",
       "             City        Country Latitude Longitude  \n",
       "402597  Arlington  United States   32.95N    96.70W  \n",
       "405836  Arlington  United States   39.38N    76.99W  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the relevance of \"latitude\" and \"longitude\" columns\n",
    "# Checking for a particular city and date\n",
    "df_global_temperature[(df_global_temperature.City == 'Arlington') & (df_global_temperature.dt == '1950-02-01')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "From the above analysis for Arlington city, we see that temperature is measured in different locations for each city. Hence we will also include \"latitude\" and \"longitude\" columns in our data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Exploring U.S. City Demographic dataset: us-cities-demographics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                       0\n",
       "State                      0\n",
       "Median Age                 0\n",
       "Male Population            3\n",
       "Female Population          3\n",
       "Total Population           0\n",
       "Number of Veterans        13\n",
       "Foreign-born              13\n",
       "Average Household Size    16\n",
       "State Code                 0\n",
       "Race                       0\n",
       "Count                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking for missing values in this dataset\n",
    "df_demographics.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This dataset looks pretty clean with much few missing values hence no much cleaning is required apart from changing city and state columns to upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Wilmington</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>36.4</td>\n",
       "      <td>32680.0</td>\n",
       "      <td>39277.0</td>\n",
       "      <td>71957</td>\n",
       "      <td>3063.0</td>\n",
       "      <td>3336.0</td>\n",
       "      <td>2.45</td>\n",
       "      <td>DE</td>\n",
       "      <td>Asian</td>\n",
       "      <td>1193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Lakewood</td>\n",
       "      <td>California</td>\n",
       "      <td>39.9</td>\n",
       "      <td>41523.0</td>\n",
       "      <td>40069.0</td>\n",
       "      <td>81592</td>\n",
       "      <td>4094.0</td>\n",
       "      <td>18274.0</td>\n",
       "      <td>3.13</td>\n",
       "      <td>CA</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>24987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Glendale</td>\n",
       "      <td>California</td>\n",
       "      <td>42.1</td>\n",
       "      <td>98181.0</td>\n",
       "      <td>102844.0</td>\n",
       "      <td>201025</td>\n",
       "      <td>4448.0</td>\n",
       "      <td>111510.0</td>\n",
       "      <td>2.69</td>\n",
       "      <td>CA</td>\n",
       "      <td>White</td>\n",
       "      <td>146718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>Springfield</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>31.8</td>\n",
       "      <td>74744.0</td>\n",
       "      <td>79592.0</td>\n",
       "      <td>154336</td>\n",
       "      <td>5723.0</td>\n",
       "      <td>16226.0</td>\n",
       "      <td>2.81</td>\n",
       "      <td>MA</td>\n",
       "      <td>Asian</td>\n",
       "      <td>5606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>Bloomington</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>23.5</td>\n",
       "      <td>40588.0</td>\n",
       "      <td>43227.0</td>\n",
       "      <td>83815</td>\n",
       "      <td>2368.0</td>\n",
       "      <td>10033.0</td>\n",
       "      <td>2.33</td>\n",
       "      <td>IN</td>\n",
       "      <td>Asian</td>\n",
       "      <td>9801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            City          State  Median Age  Male Population  \\\n",
       "177   Wilmington       Delaware        36.4          32680.0   \n",
       "210     Lakewood     California        39.9          41523.0   \n",
       "238     Glendale     California        42.1          98181.0   \n",
       "300  Springfield  Massachusetts        31.8          74744.0   \n",
       "549  Bloomington        Indiana        23.5          40588.0   \n",
       "\n",
       "     Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "177            39277.0             71957              3063.0        3336.0   \n",
       "210            40069.0             81592              4094.0       18274.0   \n",
       "238           102844.0            201025              4448.0      111510.0   \n",
       "300            79592.0            154336              5723.0       16226.0   \n",
       "549            43227.0             83815              2368.0       10033.0   \n",
       "\n",
       "     Average Household Size State Code                Race   Count  \n",
       "177                    2.45         DE               Asian    1193  \n",
       "210                    3.13         CA  Hispanic or Latino   24987  \n",
       "238                    2.69         CA               White  146718  \n",
       "300                    2.81         MA               Asian    5606  \n",
       "549                    2.33         IN               Asian    9801  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if City and Race column would help us to find unique records\n",
    "df_demographics[df_demographics[['City', 'Race']].duplicated()].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LAKEWOOD</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>37.7</td>\n",
       "      <td>76013.0</td>\n",
       "      <td>76576.0</td>\n",
       "      <td>152589</td>\n",
       "      <td>9988.0</td>\n",
       "      <td>14169.0</td>\n",
       "      <td>2.29</td>\n",
       "      <td>CO</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>33630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>LAKEWOOD</td>\n",
       "      <td>California</td>\n",
       "      <td>39.9</td>\n",
       "      <td>41523.0</td>\n",
       "      <td>40069.0</td>\n",
       "      <td>81592</td>\n",
       "      <td>4094.0</td>\n",
       "      <td>18274.0</td>\n",
       "      <td>3.13</td>\n",
       "      <td>CA</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>24987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         City       State  Median Age  Male Population  Female Population  \\\n",
       "27   LAKEWOOD    Colorado        37.7          76013.0            76576.0   \n",
       "210  LAKEWOOD  California        39.9          41523.0            40069.0   \n",
       "\n",
       "     Total Population  Number of Veterans  Foreign-born  \\\n",
       "27             152589              9988.0       14169.0   \n",
       "210             81592              4094.0       18274.0   \n",
       "\n",
       "     Average Household Size State Code                Race  Count  \n",
       "27                     2.29         CO  Hispanic or Latino  33630  \n",
       "210                    3.13         CA  Hispanic or Latino  24987  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analysing a specific combination of City='LAKEWOOD' AND Race ='Hispanic or Latino'\n",
    "df_demographics[(df_demographics.City == 'LAKEWOOD') & (df_demographics.Race == 'Hispanic or Latino')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Studied that, the difference between these two records is in the State column, hence we add State also to identify unique records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [City, State, Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size, State Code, Race, Count]\n",
       "Index: []"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After adding State also, we check if any data is duplicated\n",
    "df_demographics[df_demographics[['City', 'State', 'Race']].duplicated()].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Hence we will use City, State and Race for getting non duplicated record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Cleaning and Creating fact and dimension tables from Immigration dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **Fact table**\n",
    "##### ***fact_immigration_i94:***\n",
    "* Create fact_immigration_i94 table by extracting the below mentioned columns:\n",
    "    * 'cicid'\n",
    "    * 'i94yr'\n",
    "    * 'i94mon'\n",
    "    * 'i94port'\n",
    "    * 'i94addr'\n",
    "    * 'arrdate'\n",
    "    * 'depdate'\n",
    "    * 'i94mode'\n",
    "    * 'i94visa'\n",
    "* Change the column names to an understandable format like mentioned below:\n",
    "    * 'cic_id'\n",
    "    * 'i94_year'\n",
    "    * 'i94_month'\n",
    "    * 'city_code'\n",
    "    * 'state_code'\n",
    "    * 'arrival_date'\n",
    "    * 'departure_date'\n",
    "    * 'i94_mode'\n",
    "    * 'i94_visa'\n",
    "* Cast these columns to its preferred datatype\n",
    "* Drop all the duplicate values\n",
    "* Add a country column with value 'United States'\n",
    "* Transform arrival_date, departure_date from SAS time format to \"YYYY-mm-dd\" format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **Dimension tables**\n",
    "##### ***dim_immigration_personal:***\n",
    "* Create dim_immigration_personal table by extracting the below mentioned columns:\n",
    "    * 'cicid'\n",
    "    * 'i94cit'\n",
    "    * 'i94res'\n",
    "    * 'biryear'\n",
    "    * 'gender'\n",
    "* Change the column names to an understandable form like mentioned below:\n",
    "    * 'cic_id'\n",
    "    * 'citizen_of_country'\n",
    "    * 'country_of_residence'\n",
    "    * 'birth_year'\n",
    "    * 'gender'\n",
    "* Cast these columns to its preferred datatype\n",
    "* Drop all the duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### ***dim_immigration_airline:***\n",
    "* Create dim_immigration_airline table by extracting the below mentioned columns:\n",
    "    * 'cicid'\n",
    "    * 'airline'\n",
    "    * 'admnum'\n",
    "    * 'fltno'\n",
    "    * 'visatype'\n",
    "* Change the column names to an understandable form like mentioned below:\n",
    "    * 'cic_id'\n",
    "    * 'airline'\n",
    "    * 'admin_num'\n",
    "    * 'flight_num'\n",
    "    * 'visa_type'\n",
    "* Cast these columns to its preferred datatype\n",
    "* Drop all the duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Cleaning and Creating dimension table from World Temperature Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### ***dim_temperature:***\n",
    "* Create dim_global_temperature table by extracting the below mentioned columns:\n",
    "    * 'dt'\n",
    "    * 'AverageTemperature'\n",
    "    * 'AverageTemperatureUncertainty'\n",
    "    * 'City'\n",
    "    * 'Country'\n",
    "    * 'Latitude'\n",
    "    * 'Longitude'\n",
    "* Filter just the \"United States\" data\n",
    "* Change the column names to an understandable form like mentioned below:\n",
    "    * 'date'\n",
    "    * 'avg_temp'\n",
    "    * 'avg_temp_uncertainty'\n",
    "    * 'city'\n",
    "    * 'country'\n",
    "    * 'latitude'\n",
    "    * 'longitude'\n",
    "* Convert the date to date format and all other columns to preferred datatype\n",
    "* Add year column which will be useful for querying\n",
    "* Drop all the duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Cleaning and Creating dimension table from U.S. City Demographic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### ***dim_demographics:***\n",
    "* Create dim_demographic table by extracting the below mentioned columns:\n",
    "    * 'City'\n",
    "    * 'State'\n",
    "    * 'Male Population'\n",
    "    * 'Female Population'\n",
    "    * 'Total Population'\n",
    "    * 'Number of Veterans'\n",
    "    * 'Foreign-born'\n",
    "    * 'Race'\n",
    "    * 'Median Age'\n",
    "    * 'Average Household Size'\n",
    "* Change the column names to an understandable form like mentioned below:\n",
    "    * 'city'\n",
    "    * 'state'\n",
    "    * 'male_population'\n",
    "    * 'female_population'\n",
    "    * 'total_population'\n",
    "    * 'num_of_vetarans'\n",
    "    * 'foreign_born'\n",
    "    * 'race'\n",
    "    * 'median_age'\n",
    "    * 'avg_household_size'\n",
    "* Change city and state columns to upper case\n",
    "* Change the columns to preferred datatype\n",
    "* Drop all the duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Parse I94_SAS_Labels_Descriptions.SAS Dataset to create the below mentioned look up tables:\n",
    "* ***country_code:***\n",
    "    * 'code'\n",
    "    * 'country'\n",
    "\n",
    "\n",
    "* ***city_code:***\n",
    "    * 'code'\n",
    "    * 'city'\n",
    "\n",
    "\n",
    "* ***state_code:***\n",
    "    * 'code'\n",
    "    * 'state'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "These cleaning steps have been taken care of in the **etl_spark_process.py** file. Please refer to this file for the detailed code relating to cleaning and transforming the raw data sources.This notebook is for the documentation purpose whereas the actual ETL data pipeline and data model creation is within the etl_spark_process.py file for the purpose of code reuseability and modularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "As discussed in the Project Summary, the scope of the project is to create spark-warehouse for making it easy for OLAP and Business Intelligence app to query with high performance and with much ease, our data model follows a star schema data modeling. \n",
    "\n",
    "For example, the Business Intelligence could join the fact_immigration_i94 and dim_immigration_personal, dim_immigration_airline tables for aggregating the data. \n",
    "\n",
    "Another join can be done with respect to aggregating the temperature and immigration data based on date, arrival_date and departure_date columns\n",
    "\n",
    "fact_immigration_i94 can also be joined with dim_demographics table based on city and state. Also state_code and city_code can be used as look up tables\n",
    "\n",
    "We have also added a year column to dim_temperature dataset which enables aggregating data based on year\n",
    "\n",
    "The Star Schema will have the following fact and dimension tables:\n",
    "* Fact table:\n",
    "    * fact_immigration_i94\n",
    "* Dimension tables:\n",
    "    * dim_immigration_personal\n",
    "    * dim_immigration_airline\n",
    "    * dim_temperature\n",
    "    * dim_demographics\n",
    "    * country_code\n",
    "    * city_code\n",
    "    * state_code\n",
    "    ![alt_text](StarSchema.png)\n",
    "This Star schema model enables to search the database schema with the minimum number of SQL JOINs and hence increase performance while querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1. Stage all the datasets as stated below:\n",
    "    * `../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat` (Udacity have already loaded this into an attached disk)\n",
    "    * `I94_SAS_Labels_Descriptions.SAS`\n",
    "    * `../../data2/GlobalLandTemperaturesByCity.csv`(Udacity have already loaded this into an attached disk)\n",
    "    * `us-cities-demographics.csv` \n",
    "    \n",
    "2. Perform data cleaning as mentioned in **Step 4: Run Pipelines to Model the Data ->Create the data model-> Staging and Data Cleaning** section\n",
    "3. Transform i94_apr16_sub.sas7bdat dataset to:\n",
    "    * **fact_immigration_i94**: fact table, partitioned by state\n",
    "    * **dimension tables:**\n",
    "        * **dim_immigration_personal:** partitioned by country_of_residence\n",
    "        * **dim_immigration_airline**\n",
    "4. Transform I94_SAS_Labels_Descriptions.SAS dataset to get lookup tables:\n",
    "    * **country_code**\n",
    "    * **city_code**\n",
    "    * **state_code** \n",
    "5. Transform GlobalLandTemperaturesByCity.csv dataset to get **dim_temperature** dimension table\n",
    "6. Transform us-cities-demographics.csv dataset to **dim_demographics** dimension table\n",
    "7. Store all the fact and dimension tables in parquet format thereby creating a spark warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Creating the data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "For creating the data model, I have used the etl_spark_process.py file. All the following steps have been taken care of while creating the data model:\n",
    "* Staging the raw data sources using Spark\n",
    "* Cleaning the datasets and splitting it to fact and dimenion tables mentioned in **Conceptual Data Model** section\n",
    "* Storing it as parquet files, thereby creating a spark warehouse\n",
    "\n",
    "The source of the code for processing I94_SAS_Labels_Descriptions.SAS is from https://knowledge.udacity.com/questions/125439."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating Spark Session\n",
      "https://repos.spark-packages.org/ added as a remote repository with the name: repo-1\n",
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/opt/spark-2.4.3-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "saurfang#spark-sas7bdat added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1083dee8-5c34-4159-b4f2-0463a8c9116a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound saurfang#spark-sas7bdat;2.1.0-s_2.11 in repo-1\n",
      "\tfound com.epam#parso;2.0.10 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api-scala_2.11;2.7 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.8 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;2.7.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-common;2.7.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;2.7.0 in central\n",
      "\tfound com.google.guava#guava;11.0.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound xmlenc#xmlenc;0.52 in central\n",
      "\tfound commons-httpclient#commons-httpclient;3.1 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.4 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound commons-net#commons-net;3.1 in central\n",
      "\tfound commons-collections#commons-collections;3.2.1 in central\n",
      "\tfound javax.servlet#servlet-api;2.5 in central\n",
      "\tfound org.mortbay.jetty#jetty;6.1.26 in central\n",
      "\tfound org.mortbay.jetty#jetty-util;6.1.26 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.9 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.9 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.2 in central\n",
      "\tfound javax.xml.stream#stax-api;1.0-2 in central\n",
      "\tfound javax.activation#activation;1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.9 in central\n",
      "\tfound asm#asm;3.2 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound net.java.dev.jets3t#jets3t;0.9.0 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.2.5 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.2.5 in central\n",
      "\tfound com.jamesmurty.utils#java-xmlbuilder;0.4 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound commons-configuration#commons-configuration;1.6 in central\n",
      "\tfound commons-digester#commons-digester;1.8 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.7.0 in central\n",
      "\tfound commons-beanutils#commons-beanutils-core;1.8.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.10 in central\n",
      "\tfound org.apache.avro#avro;1.7.4 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.0.4.1 in central\n",
      "\tfound org.apache.commons#commons-compress;1.4.1 in central\n",
      "\tfound org.tukaani#xz;1.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.2.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;2.7.0 in central\n",
      "\tfound org.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.server#apacheds-i18n;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.api#api-asn1-api;1.0.0-M20 in central\n",
      "\tfound org.apache.directory.api#api-util;1.0.0-M20 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.4.6 in central\n",
      "\tfound org.slf4j#slf4j-log4j12;1.7.10 in central\n",
      "\tfound io.netty#netty;3.6.2.Final in central\n",
      "\tfound org.apache.curator#curator-framework;2.7.1 in central\n",
      "\tfound org.apache.curator#curator-client;2.7.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.42 in central\n",
      "\tfound org.apache.curator#curator-recipes;2.7.1 in central\n",
      "\tfound org.apache.htrace#htrace-core;3.1.0-incubating in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound jline#jline;0.9.94 in central\n",
      "\tfound junit#junit;4.11 in central\n",
      "\tfound org.hamcrest#hamcrest-core;1.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.2.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk;1.7.4 in central\n",
      "\tfound joda-time#joda-time;2.10.14 in central\n",
      "\t[2.10.14] joda-time#joda-time;[2.2,)\n",
      "downloading https://repos.spark-packages.org/saurfang/spark-sas7bdat/2.1.0-s_2.11/spark-sas7bdat-2.1.0-s_2.11.jar ...\n",
      "\t[SUCCESSFUL ] saurfang#spark-sas7bdat;2.1.0-s_2.11!spark-sas7bdat.jar (29ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.0/hadoop-aws-2.7.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;2.7.0!hadoop-aws.jar (28ms)\n",
      "downloading https://repo1.maven.org/maven2/com/epam/parso/2.0.10/parso-2.0.10.jar ...\n",
      "\t[SUCCESSFUL ] com.epam#parso;2.0.10!parso.jar (16ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/2.7.0/hadoop-common-2.7.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-common;2.7.0!hadoop-common.jar (62ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.2.3/jackson-databind-2.2.3.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-databind;2.2.3!jackson-databind.jar (32ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.2.3/jackson-annotations-2.2.3.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-annotations;2.2.3!jackson-annotations.jar (15ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk;1.7.4!aws-java-sdk.jar (171ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-annotations/2.7.0/hadoop-annotations-2.7.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-annotations;2.7.0!hadoop-annotations.jar (12ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/guava/guava/11.0.2/guava-11.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.guava#guava;11.0.2!guava.jar (33ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar ...\n",
      "\t[SUCCESSFUL ] commons-cli#commons-cli;1.2!commons-cli.jar (15ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-math3;3.1.1!commons-math3.jar (56ms)\n",
      "downloading https://repo1.maven.org/maven2/xmlenc/xmlenc/0.52/xmlenc-0.52.jar ...\n",
      "\t[SUCCESSFUL ] xmlenc#xmlenc;0.52!xmlenc.jar (13ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar ...\n",
      "\t[SUCCESSFUL ] commons-httpclient#commons-httpclient;3.1!commons-httpclient.jar (14ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-codec/commons-codec/1.4/commons-codec-1.4.jar ...\n",
      "\t[SUCCESSFUL ] commons-codec#commons-codec;1.4!commons-codec.jar (17ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-io/commons-io/2.4/commons-io-2.4.jar ...\n",
      "\t[SUCCESSFUL ] commons-io#commons-io;2.4!commons-io.jar (14ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-net/commons-net/3.1/commons-net-3.1.jar ...\n",
      "\t[SUCCESSFUL ] commons-net#commons-net;3.1!commons-net.jar (16ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar ...\n",
      "\t[SUCCESSFUL ] commons-collections#commons-collections;3.2.1!commons-collections.jar (20ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar ...\n",
      "\t[SUCCESSFUL ] javax.servlet#servlet-api;2.5!servlet-api.jar (13ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar ...\n",
      "\t[SUCCESSFUL ] org.mortbay.jetty#jetty;6.1.26!jetty.jar (17ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar ...\n",
      "\t[SUCCESSFUL ] org.mortbay.jetty#jetty-util;6.1.26!jetty-util.jar (15ms)\n",
      "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar ...\n",
      "\t[SUCCESSFUL ] com.sun.jersey#jersey-core;1.9!jersey-core.jar(bundle) (16ms)\n",
      "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar ...\n",
      "\t[SUCCESSFUL ] com.sun.jersey#jersey-json;1.9!jersey-json.jar(bundle) (14ms)\n",
      "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar ...\n",
      "\t[SUCCESSFUL ] com.sun.jersey#jersey-server;1.9!jersey-server.jar(bundle) (21ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (13ms)\n",
      "downloading https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar ...\n",
      "\t[SUCCESSFUL ] log4j#log4j;1.2.17!log4j.jar(bundle) (15ms)\n",
      "downloading https://repo1.maven.org/maven2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar ...\n",
      "\t[SUCCESSFUL ] net.java.dev.jets3t#jets3t;0.9.0!jets3t.jar (15ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar ...\n",
      "\t[SUCCESSFUL ] commons-lang#commons-lang;2.6!commons-lang.jar (16ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar ...\n",
      "\t[SUCCESSFUL ] commons-configuration#commons-configuration;1.6!commons-configuration.jar (17ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.10/slf4j-api-1.7.10.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.10!slf4j-api.jar (17ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (13ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-mapper-asl;1.9.13!jackson-mapper-asl.jar (21ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/avro/avro/1.7.4/avro-1.7.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.avro#avro;1.7.4!avro.jar (15ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.protobuf#protobuf-java;2.5.0!protobuf-java.jar(bundle) (19ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.gson#gson;2.2.4!gson.jar (14ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-auth/2.7.0/hadoop-auth-2.7.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-auth;2.7.0!hadoop-auth.jar (39ms)\n",
      "downloading https://repo1.maven.org/maven2/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar ...\n",
      "\t[SUCCESSFUL ] com.jcraft#jsch;0.1.42!jsch.jar (14ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.curator#curator-client;2.7.1!curator-client.jar(bundle) (12ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.curator#curator-recipes;2.7.1!curator-recipes.jar(bundle) (14ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (17ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.htrace#htrace-core;3.1.0-incubating!htrace-core.jar (192ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.zookeeper#zookeeper;3.4.6!zookeeper.jar (20ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-compress;1.4.1!commons-compress.jar (16ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jettison#jettison;1.1!jettison.jar(bundle) (15ms)\n",
      "downloading https://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar ...\n",
      "\t[SUCCESSFUL ] com.sun.xml.bind#jaxb-impl;2.2.3-1!jaxb-impl.jar (20ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-jaxrs;1.9.13!jackson-jaxrs.jar (12ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-xc;1.9.13!jackson-xc.jar (15ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar ...\n",
      "\t[SUCCESSFUL ] javax.xml.bind#jaxb-api;2.2.2!jaxb-api.jar (12ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar ...\n",
      "\t[SUCCESSFUL ] javax.xml.stream#stax-api;1.0-2!stax-api.jar (12ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/activation/activation/1.1/activation-1.1.jar ...\n",
      "\t[SUCCESSFUL ] javax.activation#activation;1.1!activation.jar (13ms)\n",
      "downloading https://repo1.maven.org/maven2/asm/asm/3.2/asm-3.2.jar ...\n",
      "\t[SUCCESSFUL ] asm#asm;3.2!asm.jar (16ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpclient;4.2.5!httpclient.jar (16ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.2.5/httpcore-4.2.5.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpcore;4.2.5!httpcore.jar (13ms)\n",
      "downloading https://repo1.maven.org/maven2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar ...\n",
      "\t[SUCCESSFUL ] com.jamesmurty.utils#java-xmlbuilder;0.4!java-xmlbuilder.jar (13ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-digester/commons-digester/1.8/commons-digester-1.8.jar ...\n",
      "\t[SUCCESSFUL ] commons-digester#commons-digester;1.8!commons-digester.jar (13ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar ...\n",
      "\t[SUCCESSFUL ] commons-beanutils#commons-beanutils-core;1.8.0!commons-beanutils-core.jar (14ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar ...\n",
      "\t[SUCCESSFUL ] commons-beanutils#commons-beanutils;1.7.0!commons-beanutils.jar (14ms)\n",
      "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar ...\n",
      "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.3!paranamer.jar (12ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.0.4.1/snappy-java-1.0.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.0.4.1!snappy-java.jar(bundle) (22ms)\n",
      "downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.0/xz-1.0.jar ...\n",
      "\t[SUCCESSFUL ] org.tukaani#xz;1.0!xz.jar (17ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15!apacheds-kerberos-codec.jar(bundle) (23ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.curator#curator-framework;2.7.1!curator-framework.jar(bundle) (13ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.directory.server#apacheds-i18n;2.0.0-M15!apacheds-i18n.jar(bundle) (12ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.directory.api#api-asn1-api;1.0.0-M20!api-asn1-api.jar(bundle) (12ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.directory.api#api-util;1.0.0-M20!api-util.jar(bundle) (12ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-log4j12;1.7.10!slf4j-log4j12.jar (12ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty;3.6.2.Final!netty.jar(bundle) (28ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar ...\n",
      "\t[SUCCESSFUL ] javax.servlet.jsp#jsp-api;2.1!jsp-api.jar (12ms)\n",
      "downloading https://repo1.maven.org/maven2/jline/jline/0.9.94/jline-0.9.94.jar ...\n",
      "\t[SUCCESSFUL ] jline#jline;0.9.94!jline.jar (15ms)\n",
      "downloading https://repo1.maven.org/maven2/junit/junit/4.11/junit-4.11.jar ...\n",
      "\t[SUCCESSFUL ] junit#junit;4.11!junit.jar (13ms)\n",
      "downloading https://repo1.maven.org/maven2/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar ...\n",
      "\t[SUCCESSFUL ] org.hamcrest#hamcrest-core;1.3!hamcrest-core.jar (12ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.2.3/jackson-core-2.2.3.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-core;2.2.3!jackson-core.jar (16ms)\n",
      "downloading https://repo1.maven.org/maven2/joda-time/joda-time/2.10.14/joda-time-2.10.14.jar ...\n",
      "\t[SUCCESSFUL ] joda-time#joda-time;2.10.14!joda-time.jar (21ms)\n",
      ":: resolution report :: resolve 30895ms :: artifacts dl 1730ms\n",
      "\t:: modules in use:\n",
      "\tasm#asm;3.2 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk;1.7.4 from central in [default]\n",
      "\tcom.epam#parso;2.0.10 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.2.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.2.4 from central in [default]\n",
      "\tcom.google.guava#guava;11.0.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.jamesmurty.utils#java-xmlbuilder;0.4 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.42 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.9 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.7.0 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils-core;1.8.0 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.4 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.1 from central in [default]\n",
      "\tcommons-configuration#commons-configuration;1.6 from central in [default]\n",
      "\tcommons-digester#commons-digester;1.8 from central in [default]\n",
      "\tcommons-httpclient#commons-httpclient;3.1 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.1 from central in [default]\n",
      "\tio.netty#netty;3.6.2.Final from central in [default]\n",
      "\tjavax.activation#activation;1.1 from central in [default]\n",
      "\tjavax.servlet#servlet-api;2.5 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.2 from central in [default]\n",
      "\tjavax.xml.stream#stax-api;1.0-2 from central in [default]\n",
      "\tjline#jline;0.9.94 from central in [default]\n",
      "\tjoda-time#joda-time;2.10.14 from central in [default]\n",
      "\tjunit#junit;4.11 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.java.dev.jets3t#jets3t;0.9.0 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.4 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.4.1 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.curator#curator-client;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-framework;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;2.7.1 from central in [default]\n",
      "\torg.apache.directory.api#api-asn1-api;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.api#api-util;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-i18n;2.0.0-M15 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;2.7.0 from central in [default]\n",
      "\torg.apache.htrace#htrace-core;3.1.0-incubating from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.2.5 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.2.5 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api-scala_2.11;2.7 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.4.6 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.hamcrest#hamcrest-core;1.3 from central in [default]\n",
      "\torg.mortbay.jetty#jetty;6.1.26 from central in [default]\n",
      "\torg.mortbay.jetty#jetty-util;6.1.26 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.8 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.10 from central in [default]\n",
      "\torg.slf4j#slf4j-log4j12;1.7.10 from central in [default]\n",
      "\torg.tukaani#xz;1.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.0.4.1 from central in [default]\n",
      "\tsaurfang#spark-sas7bdat;2.1.0-s_2.11 from repo-1 in [default]\n",
      "\txmlenc#xmlenc;0.52 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.5 by [org.slf4j#slf4j-api;1.7.10] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   75  |   72  |   72  |   1   ||   74  |   72  |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/saurfang/spark-sas7bdat/2.1.0-s_2.11/spark-sas7bdat-2.1.0-s_2.11.pom\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/saurfang/spark-sas7bdat/2.1.0-s_2.11/spark-sas7bdat-2.1.0-s_2.11.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/saurfang/spark-sas7bdat/2.1.0-s_2.11/spark-sas7bdat-2.1.0-s_2.11-sources.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/saurfang/spark-sas7bdat/2.1.0-s_2.11/spark-sas7bdat-2.1.0-s_2.11-src.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/saurfang/spark-sas7bdat/2.1.0-s_2.11/spark-sas7bdat-2.1.0-s_2.11-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/hadoop/hadoop-main/2.7.0/hadoop-main-2.7.0.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/hadoop/hadoop-project/2.7.0/hadoop-project-2.7.0.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/hadoop/hadoop-project-dist/2.7.0/hadoop-project-dist-2.7.0.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/sonatype/oss/oss-parent/7/oss-parent-7.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/google/guava/guava-parent/11.0.2/guava-parent-11.0.2.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/apache/4/apache-4.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/commons/commons-parent/11/commons-parent-11.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/commons/commons-parent/24/commons-parent-24.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/xmlenc/xmlenc/0.52/xmlenc-0.52-sources.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/xmlenc/xmlenc/0.52/xmlenc-0.52-src.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/xmlenc/xmlenc/0.52/xmlenc-0.52-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/apache/13/apache-13.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/commons/commons-parent/28/commons-parent-28.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/commons/commons-parent/25/commons-parent-25.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/commons/commons-parent/23/commons-parent-23.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/commons/commons-parent/9/commons-parent-9.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/javax/servlet/servlet-api/2.5/servlet-api-2.5-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/eclipse/jetty/jetty-parent/14/jetty-parent-14.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/mortbay/jetty/jetty-parent/10/jetty-parent-10.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/mortbay/jetty/project/6.1.26/project-6.1.26.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/net/java/jvnet-parent/1/jvnet-parent-1.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/sun/jersey/jersey-project/1.9/jersey-project-1.9.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/javax/activation/activation/1.1/activation-1.1-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/asm/asm-parent/3.2/asm-parent-3.2.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/asm/asm/3.2/asm-3.2-sources.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/asm/asm/3.2/asm-3.2-src.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/asm/asm/3.2/asm-3.2-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/httpcomponents/project/6/project-6.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/httpcomponents/httpcomponents-client/4.2.5/httpcomponents-client-4.2.5.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/httpcomponents/project/7/project-7.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/httpcomponents/httpcomponents-core/4.2.5/httpcomponents-core-4.2.5.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/sonatype/oss/oss-parent/3/oss-parent-3.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/apache/7/apache-7.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/commons/commons-parent/17/commons-parent-17.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/commons-configuration/commons-configuration/1.6/commons-configuration-1.6-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/commons-digester/commons-digester/1.8/commons-digester-1.8-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0-sources.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0-src.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/slf4j/slf4j-parent/1.7.10/slf4j-parent-1.7.10.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/apache/10/apache-10.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/avro/avro-toplevel/1.7.4/avro-toplevel-1.7.4.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/avro/avro-parent/1.7.4/avro-parent-1.7.4.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/codehaus/codehaus-parent/1/codehaus-parent-1.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/thoughtworks/paranamer/paranamer-parent/2.3/paranamer-parent-2.3.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/google/google/1/google-1.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/apache/11/apache-11.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/directory/project/project/31/project-31.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/directory/server/apacheds-parent/2.0.0-M15/apacheds-parent-2.0.0-M15.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/directory/api/api-parent/1.0.0-M20/api-parent-1.0.0-M20.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/directory/api/api-asn1-parent/1.0.0-M20/api-asn1-parent-1.0.0-M20.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/apache/14/apache-14.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/curator/apache-curator/2.7.1/apache-curator-2.7.1.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/jcraft/jsch/0.1.42/jsch-0.1.42-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/apache/12/apache-12.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/htrace/htrace/3.1.0-incubating/htrace-3.1.0-incubating.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/jline/jline/0.9.94/jline-0.9.94-javadoc.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/hamcrest/hamcrest-parent/1.3/hamcrest-parent-1.3.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/fasterxml/oss-parent/11/oss-parent-11.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/fasterxml/oss-parent/10/oss-parent-10.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/joda-time/joda-time/maven-metadata.xml\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/joda-time/joda-time/\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/joda-time/joda-time/\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1083dee8-5c34-4159-b4f2-0463a8c9116a\n",
      "\tconfs: [default]\n",
      "\t72 artifacts copied, 2 already retrieved (36499kB/182ms)\n",
      "22/07/14 16:15:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/07/14 16:15:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "INFO:root:Processing process_i94_immigration_data function\n",
      "INFO:root:Start processing i94_apr16_sub.sas7bdat dataset\n",
      "INFO:root:Start processing fact_immigration_i94\n",
      "INFO:root:Writing to parquet files partitioned by state\n",
      "22/07/14 16:16:04 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "INFO:root:Finished writing to parquet files partitioned by state                \n",
      "INFO:root:Start processing dim_immigration_personal\n",
      "INFO:root:Writing to parquet files partitioned by country_of_residence\n",
      "INFO:root:Finished writing to parquet files                                     \n",
      "INFO:root:Start processing dim_immigration_airline\n",
      "INFO:root:Writing to parquet files\n",
      "INFO:root:Finished writing to parquet files                                     \n",
      "INFO:root:Processing process_SAS_label_descriptions_file function\n",
      "INFO:root:Start processing I94_SAS_Labels_Descriptions.SAS dataset\n",
      "INFO:root:Creating Country dict\n",
      "INFO:root:Creating City dict\n",
      "INFO:root:Creating State dict\n",
      "INFO:root:Writing country_code to parquet files\n",
      "INFO:root:Finished writing country_code to parquet file\n",
      "INFO:root:Writing city_code to parquet files\n",
      "INFO:root:Finished writing city_code to parquet file\n",
      "INFO:root:Writing state_code to parquet files\n",
      "INFO:root:Finished writing state_code to parquet file\n",
      "INFO:root:Processing process_usa_temperature_data function\n",
      "INFO:root:Start processing GlobalLandTemperaturesByCity.csv dataset\n",
      "INFO:root:Start processing dim_temperature_usa                                  \n",
      "INFO:root:Writing to parquet files\n",
      "INFO:root:Finished writing to parquet files                                     \n",
      "INFO:root:Processing process_demographics_data function\n",
      "INFO:root:Start processing us-cities-demographics.csv dataset\n",
      "INFO:root:Start processing dim_demographics\n",
      "INFO:root:Writing to parquet files\n",
      "INFO:root:Finished writing to parquet                                           \n",
      "INFO:root:Data processing completed\n"
     ]
    }
   ],
   "source": [
    "# Creating the Star Schema data model\n",
    "!python3 etl_spark_process.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "* **Primary Data Quality Checks:**\n",
    "* Verifying that data schema of all fact and dimension tables after the ETL data pipeline, matches the conceptual data model\n",
    "* Verifying there are no empty tables after the ETL data pipeline process\n",
    "\n",
    "* **Secondary Data Quality Checks**\n",
    "* Verify the number of partitions in dim_immigration_personal folder, there should be 229 partitions\n",
    "* Verifying the number of partitions in fact_immigration_i94 folder, there should be 457 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: fact_immigration_i94\n",
      "root\n",
      " |-- cic_id: integer (nullable = true)\n",
      " |-- i94_year: integer (nullable = true)\n",
      " |-- i94_month: integer (nullable = true)\n",
      " |-- city_code: string (nullable = true)\n",
      " |-- arrival_date: string (nullable = true)\n",
      " |-- departure_date: string (nullable = true)\n",
      " |-- i94_mode: integer (nullable = true)\n",
      " |-- i94_visa: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      "\n",
      "Table: dim_immigration_personal\n",
      "root\n",
      " |-- cic_id: integer (nullable = true)\n",
      " |-- citizen_of_country: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- country_of_residence: integer (nullable = true)\n",
      "\n",
      "Table: dim_immigration_airline\n",
      "root\n",
      " |-- cic_id: integer (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admin_num: integer (nullable = true)\n",
      " |-- flight_num: string (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      "\n",
      "Table: dim_temperature\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- avg_temp: double (nullable = true)\n",
      " |-- avg_temp_uncertainty: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n",
      "Table: country_code\n",
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "Table: state_code\n",
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "Table: city_code\n",
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "Table: dim_demographics\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- num_of_vetarans: integer (nullable = true)\n",
      " |-- foreign_born: integer (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- avg_household_size: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performing data quality check to verify data schema of all fact and dimension tables matches the conceptual data model\n",
    "path = ['fact_immigration_i94', 'dim_immigration_personal', 'dim_immigration_airline', 'dim_temperature', 'country_code', 'state_code', 'city_code', 'dim_demographics']\n",
    "for file in path:\n",
    "    df = spark.read.parquet(file)\n",
    "    print(\"Table: \" + file)\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_immigration_i94 table has 3096313 records\n",
      "dim_immigration_personal table has 3096313 records\n",
      "dim_immigration_airline table has 3096313 records\n",
      "dim_temperature table has 687289 records\n",
      "country_code table has 289 records\n",
      "state_code table has 55 records\n",
      "city_code table has 660 records\n",
      "dim_demographics table has 2891 records\n"
     ]
    }
   ],
   "source": [
    "# Performing data quality check to verify there are no empty tables after the ETL data pipeline process\n",
    "path = ['fact_immigration_i94', 'dim_immigration_personal', 'dim_immigration_airline', 'dim_temperature', 'country_code', 'state_code', 'city_code','dim_demographics']\n",
    "for file in path:\n",
    "    df = spark.read.parquet(str(file))\n",
    "    num_records = df.count()\n",
    "    if num_records <= 0:\n",
    "        raise ValueError(\"This table is empty!\")\n",
    "    else:\n",
    "        print(file + \" table\" + f\" has {num_records} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration.createOrReplaceTempView(\"raw_immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT i94addr)|\n",
      "+-----------------------+\n",
      "|                    457|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verifying number of unique i94addr which is mapped to \"state_code\" in our resultant \"fact_immigration_i94\" table\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT i94addr)\n",
    "FROM raw_immigration_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions based on state_code is 457\n"
     ]
    }
   ],
   "source": [
    "# Checking the number of partitions in \"fact_immigration_i94\" folder\n",
    "# There should be 457 partitions\n",
    "file_path = \"fact_immigration_i94\"\n",
    "totalDir = 0\n",
    "for base, dirs, files in os.walk(file_path):\n",
    "    for directories in dirs:\n",
    "        if directories != 'state_code=__HIVE_DEFAULT_PARTITION__':\n",
    "            totalDir += 1\n",
    "print(f\"Number of partitions based on state_code is {totalDir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT i94res)|\n",
      "+----------------------+\n",
      "|                   229|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verifying number of unique i94res which is mapped to \"country_of_residence\" in our resultant \"dim_immigration_personal\" table\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT i94res)\n",
    "FROM raw_immigration_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions based on country_of_residence is 229\n"
     ]
    }
   ],
   "source": [
    "# Verifying the number of partitions in dim_immigration_personal\n",
    "# There should be 229 partitions\n",
    "file_path = \"dim_immigration_personal\"\n",
    "totalDir = 0\n",
    "for base, dirs, files in os.walk(file_path):\n",
    "    for directories in dirs:\n",
    "        totalDir += 1\n",
    "print(f\"Number of partitions based on country_of_residence is {totalDir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Please refer below for the data dictionary:\n",
    "![fact_immigration_i94.png](fact_immigration_i94.png)\n",
    "![dim_immigration_personal.png](dim_immigration_personal.png)\n",
    "![dim_immigration_airline.png](dim_immigration_airline.png)\n",
    "![country_state_city_code.png](country_state_city_code.png)\n",
    "![dim_temperature.png](dim_temperature.png)\n",
    "![dim_demographics.png](dim_demographics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Project Write Up\n",
    "* **Rationale for the choice of tools and technologies:**\n",
    "    * For performing Exploratory data analysis on small dataset, Pandas library is used. It is helpful to efficiently load and manipulate data\n",
    "    * For performing Exploratory data analysis on large dataset, Pyspark library is used. We can also think of using Spark available within Amazon EMR for distributed processing\n",
    "    * Jupyter Notebook is used to show the data structure and the need for data cleaning. Language used is Python since I am most comfortable with this language. For assessing the data, I have also used SQL language.\n",
    "    * configparser python 3 is needed to run the python scripts\n",
    "* **Frequency of Data Source:**\n",
    "    * Since the raw dataset for immigration and temperature is built monthly, tables relating to these should be updated monthly\n",
    "    * Since demography data collection takes time and high frequent demography might take high cost, tables relating to demography dataset could be updated annually\n",
    "* **Future Design Considerations:**\n",
    "     * The data was increased by 100x:\n",
    "        * As the size of data source increases, store the raw datasourses and the parquet files(obtained after ETL data pipeline) in Amazon S3. \n",
    "        * Deploy this Spark solution on a cluster using AWS EMR cluster. AWS will easily scale when data increases by 100x.\n",
    "     * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "        * Use Apache Airflow to run this on a schedule (use schedul_interval='daily'). Apache Airflow will take care of building ETL data pipeline to regularly update the data and populate a report.\n",
    "        * Airflow can also integrate with Python and AWS very well\n",
    "     * The database needed to be accessed by 100+ people.\n",
    "        * AWS Redshift can be used since it has the capability of massive parallel processing and limitless concurrency to handle thousands of concurrent queries executed by users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Sample analysis using the created Data Model:**\n",
    "\n",
    "Following is one of the sample analysis which could be performed with the created data model. This analyzes if the states with the most immigrants are also the states with more foreign-born people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Staging the fact_immigration_i94 table \n",
    "fact_immigration_analysis = spark.read.parquet('fact_immigration_i94')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_immigration_analysis.createOrReplaceTempView('fact_immigration_i94_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Staging the dim_demographics table\n",
    "dim_demographics_analysis = spark.read.parquet('dim_demographics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_demographics_analysis.createOrReplaceTempView('dim_demographics_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Staging the state_code table\n",
    "state_code_analysis = spark.read.parquet(\"state_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_code_analysis.createOrReplaceTempView(\"state_code_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Joining dim_demographics_tbl with state_code_tbl to fetch the state_code column\n",
    "spark.sql(\"\"\"\n",
    "SELECT dim_demographics_tbl.*, \n",
    "state_code_tbl.code AS state_code\n",
    "FROM dim_demographics_tbl\n",
    "JOIN state_code_tbl ON dim_demographics_tbl.state = state_code_tbl.state \n",
    "\"\"\").createOrReplaceTempView(\"dim_demographics_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating a view with info of immigrants count per state\n",
    "spark.sql(\"\"\"\n",
    "SELECT state_code, COUNT(cic_id) AS immigrants_count\n",
    "FROM fact_immigration_i94_tbl\n",
    "GROUP BY state_code\n",
    "ORDER BY immigrants_count DESC\n",
    "\"\"\").createOrReplaceTempView(\"immigrants_count_per_state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating a view with info of total foreign born per state\n",
    "spark.sql(\"\"\"\n",
    "SELECT state_code, SUM(foreign_born) AS total_foreign_born\n",
    "FROM dim_demographics_tbl\n",
    "GROUP BY state_code\n",
    "ORDER BY total_foreign_born DESC\n",
    "\"\"\").createOrReplaceTempView(\"total_foreign_born_per_state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+------------------+-------------------------------+\n",
      "|state_code|immigrants_count|total_foreign_born|immigrant_to_foreign_born_ratio|\n",
      "+----------+----------------+------------------+-------------------------------+\n",
      "|        FL|          621701|           7845566|                           7.92|\n",
      "|        NY|          553677|          17186873|                           3.22|\n",
      "|        CA|          470386|          37059662|                           1.27|\n",
      "|        HI|          168764|            506560|                          33.32|\n",
      "|        TX|          134321|          14498054|                           0.93|\n",
      "|        NV|          114609|           2406685|                           4.76|\n",
      "|        IL|           82126|           4632600|                           1.77|\n",
      "|        NJ|           76531|           2327750|                           3.29|\n",
      "|        MA|           70486|           2573815|                           2.74|\n",
      "|        WA|           55792|           2204810|                           2.53|\n",
      "|        GA|           44663|            738925|                           6.04|\n",
      "|        MI|           32101|           1214547|                           2.64|\n",
      "|        VA|           31399|           1346270|                           2.33|\n",
      "|        PA|           30293|           1439936|                            2.1|\n",
      "|        NE|           26574|            356105|                           7.46|\n",
      "|        MD|           25360|           1148970|                           2.21|\n",
      "|        LA|           22655|            417095|                           5.43|\n",
      "|        AZ|           20218|           3411565|                           0.59|\n",
      "|        OH|           18089|            873891|                           2.07|\n",
      "|        CO|           15874|           1688155|                           0.94|\n",
      "+----------+----------------+------------------+-------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining the above two views for the purpose of analysing \n",
    "# number of immigrants per state versus number of foreign born per state and its related percentage\n",
    "spark.sql(\"\"\"\n",
    "SELECT imm.state_code, immigrants_count, total_foreign_born,\n",
    "ROUND((immigrants_count/total_foreign_born)*100,2) AS immigrant_to_foreign_born_ratio\n",
    "FROM immigrants_count_per_state imm\n",
    "JOIN total_foreign_born_per_state fb ON imm.state_code = fb.state_code\n",
    "ORDER BY immigrants_count DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Even though immigration dataset had approx 3m records, I could join this with demographics dataset to analyze the immigrants count versus foreign born count with much less query run time.\n",
    "\n",
    "Based on the above data, we could assume the following:\n",
    "* Though California, New York and Texas have the maximum foreign-born people due to the cosmopolitan culture, we could understand from the dataset that most of the immigrants are settled in Florida.\n",
    "* States like Florida, Georgia, Nevada, etc. are having better immigrant to foreign born ratio compared to the other populous states. (Hawaii is an exception because of the cultural difference due to its late annexation to the US confederation)\n",
    "* This could mean that immigrant settlement in a specific State might not only depend on the existing foreign-born population there.\n",
    "* There could be other factors that may drive the immigration; better job opportunities, cost of living, healthcare benefits etc. But this may need additional data sources to conclude.\n",
    "\n",
    "However more meaningful conclusion about the immigrant behavior could only be derived after analyzing better granular data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
